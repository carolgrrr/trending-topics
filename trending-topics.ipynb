{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find trending topics on twitter for a list of locations and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from TwitterAPI import TwitterAPI\n",
    "import sys\n",
    "import time\n",
    "#import yagmail\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_twitter(config_file):\n",
    "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
    "    Args:\n",
    "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    twitter = TwitterAPI(\n",
    "                   config.get('twitter', 'consumer_key'),\n",
    "                   config.get('twitter', 'consumer_secret'),\n",
    "                   config.get('twitter', 'access_token'),\n",
    "                   config.get('twitter', 'access_token_secret'))\n",
    "    return twitter\n",
    "\n",
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      resource ... A resource string to request.\n",
    "      params ..... A parameter dictionary for the request.\n",
    "      max_tries .. The maximum number of tries to attempt.\n",
    "    Returns:\n",
    "      A TwitterResponse object, or None if failed.\n",
    "    \"\"\"\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print('Got error:', request.text, '\\nsleeping for 15 minutes.', file=sys.stderr)\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)\n",
    "\n",
    "def find_trends(twitter, location):\n",
    "    topics = robust_request(twitter, 'trends/place', {'id': location}, 20)\n",
    "    trends = []\n",
    "    for t in topics:\n",
    "        topic = \"%d\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(location, t['name'], t['url'], str(t['tweet_volume']), t['promoted_content'], t['query'])\n",
    "        trends.append(topic)\n",
    "    return trends\n",
    "\n",
    "def find_place_ids(twitter):\n",
    "    places = robust_request(twitter, 'trends/available',{}, 20)\n",
    "    place_ids = []\n",
    "    for p in places:\n",
    "        place_ids.append(p['woeid'])\n",
    "    return place_ids\n",
    "\n",
    "def find_places(twitter):\n",
    "    places = robust_request(twitter, 'trends/available',{}, 20)\n",
    "    all_places = []\n",
    "    for p in places:\n",
    "        all_places.append(p)\n",
    "    return all_places\n",
    "\n",
    "def extract_topics(infile, outfile, keyword):\n",
    "    topics = []\n",
    "    with open(infile, 'r') as tsv_infile:\n",
    "        for line in tsv_infile:\n",
    "            row = line.split()\n",
    "            if keyword in row[2]:\n",
    "                topics.append(row)\n",
    "    \n",
    "    with open(outfile, 'w') as tsv_outfile:\n",
    "        tsv_outfile.write('Location Name\\tWOE ID\\tName\\tURL\\tEvents\\tPromoted?\\tQuery\\n')\n",
    "        for topic in topics:\n",
    "            row = \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(topic[0], topic[1], topic[2], topic[3], topic[4], topic[5], topic[6])\n",
    "            tsv_outfile.write(row)\n",
    "\n",
    "    print('topics filtered.')\n",
    "\n",
    "def count_topics(filename):\n",
    "    all_topics = []\n",
    "    with open(filename, 'r') as topics:\n",
    "        for line in topics:\n",
    "            row = line.split('\\t')\n",
    "            all_topics.append(row[2])\n",
    "\n",
    "    topic_count = Counter(all_topics)\n",
    "    return topic_count\n",
    "\n",
    "def email_file(config, filename):\n",
    "    from_addr = config.get('email', 'from')\n",
    "    passwrd = config.get('email', 'pass')\n",
    "    to_addr = config.get('email', 'to')\n",
    "\n",
    "    contents = ['See attached.', filename]\n",
    "\n",
    "    yag = yagmail.SMTP(from_addr, passwrd)\n",
    "    yag.send(to_addr, filename, contents)\n",
    "    print('email sent.')\n",
    "\n",
    "def get_datestring():\n",
    "    today = datetime.today()\n",
    "    year, month, day = today.year, today.month, today.day\n",
    "\n",
    "    if len(str(month)) < 2:\n",
    "        month = \"0%d\" %(month)\n",
    "    else:\n",
    "        month = \"%d\" %(month)\n",
    "    \n",
    "    if len(str(day)) < 2:\n",
    "        day = \"0%d\" %(day)\n",
    "    else:\n",
    "        day = \"%d\" %(day)\n",
    "\n",
    "    datestring = \"%d-%s-%s\" %(year, month, day)\n",
    "    return datestring\n",
    "\n",
    "def get_trending_topics(filename):\n",
    "    with open(all_topics, 'w') as tsv_file:\n",
    "        tsv_file.write('Location\\tWOE ID\\tName\\tURL\\tEvents\\tPromoted?\\tQuery\\n')\n",
    "\n",
    "    # iterate through all twitter locations \n",
    "    # store trending topics for each location\n",
    "    for pid in place_ids:\n",
    "        try:\n",
    "            trends = find_trends(twitter, pid)\n",
    "        \n",
    "            for p in places:\n",
    "                if p['woeid'] == pid:\n",
    "                    name = p['name']\n",
    "            with open(all_topics, 'a') as tsv_file:\n",
    "                for topic in trends:\n",
    "                    tsv_file.write(name+'\\t'+ topic)\n",
    "        except (Timeout, ssl.SSLError, ReadTimeoutError, ConnectionError) as exc:\n",
    "            print(\"error: %s\" % exc)\n",
    "            sleep(60*5)\n",
    "            \n",
    "def get_top_topics(filename):\n",
    "    topic_counter = count_topics(filename)\n",
    "    top_topics = []\n",
    "    with open(filename, 'r') as topics:\n",
    "        for line in topics:\n",
    "            row = line.split('\\t')\n",
    "            loc, woe, name, events, promoted = row[0], row[1], row[2], row[4], row[5]\n",
    "            count = topic_counter[name]\n",
    "            top_topics.append((loc, woe, name, events, promoted, count))\n",
    "    sorted_topics = sorted(top_topics, key=lambda x: (x[5], x[2]), reverse=True)\n",
    "    top_filename = \"top-\" + filename\n",
    "    with open(top_filename, 'w') as tsv_file:\n",
    "        tsv_file.write('Location\\tWOE ID\\tName\\tEvents\\tPromoted?\\tCount\\n')\n",
    "    \n",
    "        for topic in sorted_topics:\n",
    "            if topic[5] > 1:\n",
    "                row = \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(topic[0], topic[1], topic[2], topic[3], topic[4], topic[5])\n",
    "                tsv_file.write(row)\n",
    "\n",
    "def add_regions(original_file, region_file):\n",
    "    topics_with_regions = []\n",
    "    region_list = []\n",
    "    with open(region_file, 'r') as regions:\n",
    "        next(regions)\n",
    "        for line in regions:\n",
    "            line = line.strip('\\n')\n",
    "            row = line.split(',')\n",
    "            region_list.append(row)\n",
    "\n",
    "    with open(original_file, 'r') as topics:\n",
    "        next(topics)\n",
    "        for line in topics:\n",
    "            line = line.strip('\\n')\n",
    "            row = line.split('\\t')\n",
    "            loc = row[0]\n",
    "            for region in region_list:\n",
    "                new_loc = region[0]\n",
    "                if loc == new_loc:\n",
    "                    row.extend([region[1], region[2], region[3], region[4]])\n",
    "                    topics_with_regions.append(row)\n",
    "    \n",
    "    reg_filename = \"regions-and-\" + original_file\n",
    "    with open(reg_filename, 'w') as tsv_file:\n",
    "        tsv_file.write('Location\\tWOE ID\\tName\\tEvents\\tPromoted?\\tCount\\tLatitude\\tLongitude\\tNation\\tRegion\\n')     \n",
    "\n",
    "        for topic in topics_with_regions:\n",
    "            #print(topic)\n",
    "            row = \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(topic[0], topic[1], topic[2], topic[3], topic[4], topic[5], topic[6], topic[7], topic[8], topic[9])\n",
    "            tsv_file.write(row)\n",
    "            \n",
    "    print(\"regions added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.cfg')\n",
    "\n",
    "filter_term = config.get('files', 'filter_term')\n",
    "prefix = config.get('files', 'prefix')\n",
    "twitter = get_twitter('settings.cfg')\n",
    "\n",
    "place_ids = find_place_ids(twitter)\n",
    "places = find_places(twitter)\n",
    "\n",
    "datestring = get_datestring()\n",
    "\n",
    "all_topics = prefix + '-' + datestring + '.csv'\n",
    "filtered_topics = prefix + '-' + filter_term + '-' + datestring + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regions added.\n"
     ]
    }
   ],
   "source": [
    "regions = 'region-locations.csv'\n",
    "infile = 'trending-topics-17-2017-03-08.csv'\n",
    "\n",
    "add_regions(infile, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_counter = count_topics(all_topics)\n",
    "#print(topic_counter.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(topic_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'trending-topics-2017-02-08.csv'\n",
    "top_topics = []\n",
    "with open(filename, 'r') as topics:\n",
    "    for line in topics:\n",
    "        row = line.split('\\t')\n",
    "        loc, woe, name, events, promoted = row[0], row[1], row[2], row[4], row[5]\n",
    "        count = topic_counter[name]\n",
    "        #print(count)\n",
    "        top_topics.append((loc, woe, name, events, promoted, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_topics = sorted(top_topics, key=lambda x: (x[5], x[2]), reverse=True)\n",
    "#print(sorted_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_filename = 'top-trending-topics-2017-02-08.csv'\n",
    "\n",
    "with open(top_filename, 'w') as tsv_file:\n",
    "    tsv_file.write('Location Name\\tWOE ID\\tName\\tEvents\\tPromoted?\\tCount\\n')\n",
    "    \n",
    "    for topic in sorted_topics:\n",
    "        if topic[5] > 1:\n",
    "            row = \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(topic[0], topic[1], topic[2], topic[3], topic[4], topic[5])\n",
    "            tsv_file.write(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_trending_topics(all_topics)\n",
    "extract_topics(all_topics, filtered_topics, filter_term)\n",
    "email_file(config, filtered_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter = get_twitter('settings.cfg')\n",
    "print('Established Twitter connection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.cfg')\n",
    "all_topics = config.get('files', 'all_topics')\n",
    "filtered_topics = config.get('files', 'all_topics')\n",
    "filter_term = config.get('files', 'filter_term')\n",
    "\n",
    "place_ids = find_place_ids(twitter)\n",
    "places = find_places(twitter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(all_topics, 'w') as tsv_file:\n",
    "    tsv_file.write('Location Name\\tWOE ID\\tName\\tURL\\tEvents\\tPromoted?\\tQuery\\n')\n",
    "\n",
    "for pid in place_ids:\n",
    "    try:\n",
    "        trends = find_trends(twitter, pid)\n",
    "        \n",
    "        for p in places:\n",
    "            if p['woeid'] == pid:\n",
    "                name = p['name']\n",
    "        with open('all_topics', 'a') as tsv_file:\n",
    "            #print(trends[0])\n",
    "            for topic in trends:\n",
    "                tsv_file.write(name+'\\t'+ topic)\n",
    "    except (Timeout, ssl.SSLError, ReadTimeoutError, ConnectionError) as exc:\n",
    "        print(\"error: %s\" % exc)\n",
    "        sleep(60*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started at 8:31 PM. Failed, then restarted at 8:33 PM. Failed at 8:55 & restarted.\n",
    "\n",
    "Started at 10:58. Reached rate limit at 11:00\n",
    "Started at 12:49am. Failed at 3:10am (almost done)\n",
    "Started at 4:24am\n",
    "\n",
    "Started at 3PM 1/7. Done before 5PM (after 4:30 I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "with open(filtered_topics, 'r') as tsv_file:\n",
    "    for line in tsv_file:\n",
    "        row = line.split()\n",
    "        if filter_term in row[2]:\n",
    "            filtered.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_topics(infile, outfile, keyword):\n",
    "    topics = []\n",
    "    with open(infile, 'r') as tsv_file:\n",
    "        for line in tsv_file:\n",
    "            row = line.split()\n",
    "            if keyword in row[2]:\n",
    "                topics.append(row)\n",
    "    return topics\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(topics_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filtered, 'w') as tsv_file:\n",
    "    for topic in topics_17:\n",
    "        row = \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" %(topic[0], topic[1], topic[2], topic[3], topic[4], topic[5], topic[6])\n",
    "        tsv_file.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today()\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year, month, day = today.year, today.month, today.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(str(month)) < 2:\n",
    "    month = \"0%d\" %(month)\n",
    "else:\n",
    "    month = \"%d\" %(month)\n",
    "    \n",
    "if len(str(day)) < 2:\n",
    "    day = \"0%d\" %(day)\n",
    "else:\n",
    "    day = \"%d\" %(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datestring = \"%d-%s-%s\" %(year, month, day)\n",
    "datestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.cfg')\n",
    "\n",
    "email_addresses = config.get('email', 'to')\n",
    "email_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_list = email_addresses.split(',')\n",
    "\n",
    "for address in email_list:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordpress_xmlrpc import Client, WordPressPost\n",
    "from wordpress_xmlrpc.methods.posts import GetPosts, NewPost\n",
    "from wordpress_xmlrpc.methods.users import GetUserInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordpress_xmlrpc import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
